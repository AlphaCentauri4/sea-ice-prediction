{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM,TimeDistributed\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96809f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enabling Dynamic Memory Allocation\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb150c5",
   "metadata": {},
   "source": [
    "## Sea Ice Prediction - MT-IceNet\n",
    "\n",
    "### Loading Rolling Window Data 1979-2021\n",
    "\n",
    "\n",
    "#### Features:\n",
    "longwave, rain_rate, snow_rate, sst, sea_ice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(\"data/X_train_rolling_filled_5f.npy\")\n",
    "y_train = np.load(\"data/y_train_rolling_filled_final.npy\")\n",
    "x_test = np.load(\"data/X_test_rolling_filled_5f.npy\")\n",
    "y_test = np.load(\"data/y_test_rolling_filled_final.npy\")\n",
    "land_mask = np.load(\"data/y_land_mask_actual.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02da8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = np.load(\"data/X_train_convlstm_rolling_biweekly_5f.npy\")\n",
    "x_test2 = np.load(\"data/X_test_convlstm_rolling_biweekly_5f.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43435717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train.shape:',x_train.shape)\n",
    "print('x_train2.shape:',x_train2.shape)\n",
    "print('y_train.shape:',y_train.shape)\n",
    "\n",
    "print('x_test.shape:',x_test.shape)\n",
    "print('x_test2.shape:',x_test2.shape)\n",
    "print('y_test.shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d45e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 6\n",
    "x_train = x_train[:-lag,:,:,:,:]\n",
    "x_test = x_test[:-lag,:,:,:,:]\n",
    "\n",
    "x_train2 = x_train2[:-(lag+1),:,:,:,:]\n",
    "x_test2 = x_test2[:-(lag+1),:,:,:,:]\n",
    "\n",
    "y_train = y_train[lag:,:,:,:]\n",
    "y_test = y_test[lag:,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20be84be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train.shape:',x_train.shape)\n",
    "print('x_train2.shape:',x_train2.shape)\n",
    "print('y_train.shape:',y_train.shape)\n",
    "\n",
    "print('x_test.shape:',x_test.shape)\n",
    "print('x_test2.shape:',x_test2.shape)\n",
    "print('y_test.shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c462fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing all nans with Zeros\n",
    "x_train = np.nan_to_num(x_train)\n",
    "x_train2 = np.nan_to_num(x_train2)\n",
    "y_train = np.nan_to_num(y_train)\n",
    "x_test = np.nan_to_num(x_test)\n",
    "x_test2 = np.nan_to_num(x_test2)\n",
    "y_test = np.nan_to_num(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257faa53",
   "metadata": {},
   "source": [
    "### Reshaping Input and Target Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def reshape_features(dataset, samples, timestep, lat, lon, features):\n",
    "    print(dataset.shape)\n",
    "    X = dataset.reshape(samples, timestep, lat, lon, features)\n",
    "    return X\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def reshape_outcome(dataset, months, lat, lon):\n",
    "    print(dataset.shape)\n",
    "    X = dataset.reshape(months, lat, lon, 1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a1f5f1",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee63348",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = len(x_train)\n",
    "NUM_TEST = len(x_test)\n",
    "print(NUM_TRAIN)\n",
    "print(NUM_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732da376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the features\n",
    "\n",
    "scaler_a = StandardScaler()\n",
    "#x_train = scaler_a.fit_transform(x_train.reshape(-1,x_train.shape[3])) #reshaping to 2d for standard scaling\n",
    "#x_test = scaler_a.transform(x_test.reshape(-1,x_test.shape[3])) #reshaping to 2d for standard scaling\n",
    "\n",
    "x_train = scaler_a.fit_transform(x_train.reshape(-1,1)) #reshaping to 2d for standard scaling\n",
    "x_test = scaler_a.transform(x_test.reshape(-1,1)) #reshaping to 2d for standard scaling\n",
    "\n",
    "scaler_b = StandardScaler()\n",
    "#x_train2 = scaler_b.fit_transform(x_train2.reshape(-1,x_train2.shape[3])) #reshaping to 2d for standard scaling\n",
    "#x_test2 = scaler_b.transform(x_test2.reshape(-1,x_test2.shape[3])) #reshaping to 2d for standard scaling\n",
    "\n",
    "x_train2 = scaler_b.fit_transform(x_train2.reshape(-1,1)) #reshaping to 2d for standard scaling\n",
    "x_test2 = scaler_b.transform(x_test2.reshape(-1,1)) #reshaping to 2d for standard scaling\n",
    "\n",
    "scaler_l = StandardScaler()\n",
    "y_train = scaler_l.fit_transform(y_train.reshape(-1,1)) #reshaping to 2d for standard scaling\n",
    "y_test = scaler_l.transform(y_test.reshape(-1,1)) #reshaping to 2d for standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec321139",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train.shape:',x_train.shape)\n",
    "print('x_train2.shape:',x_train2.shape)\n",
    "print('y_train.shape:',y_train.shape)\n",
    "\n",
    "print('x_test.shape:',x_test.shape)\n",
    "print('x_test2.shape:',x_test2.shape)\n",
    "print('y_test.shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c1e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping data to 3D for modeling\n",
    "lat = 448\n",
    "lon = 304\n",
    "timestep1 = 12\n",
    "timestep2 = 24\n",
    "features = 5\n",
    "\n",
    "x_train = reshape_features(x_train, NUM_TRAIN, timestep1, lat, lon, features) # reshaping to 3d for model\n",
    "x_test = reshape_features(x_test, NUM_TEST, timestep1, lat, lon, features) # reshaping to 3d for model\n",
    "\n",
    "x_train2 = reshape_features(x_train2, NUM_TRAIN, timestep2, lat, lon, features) # reshaping to 3d for model\n",
    "x_test2 = reshape_features(x_test2, NUM_TEST, timestep2, lat, lon, features) # reshaping to 3d for model\n",
    "\n",
    "y_train = reshape_outcome(y_train, NUM_TRAIN, lat, lon) # reshaping to 3d for model\n",
    "y_test = reshape_outcome(y_test, NUM_TEST, lat, lon) # reshaping to 3d for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d84299",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train.shape:',x_train.shape)\n",
    "print('x_train2.shape:',x_train2.shape)\n",
    "print('y_train.shape:',y_train.shape)\n",
    "\n",
    "print('x_test.shape:',x_test.shape)\n",
    "print('x_test2.shape:',x_test2.shape)\n",
    "print('y_test.shape:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse(y_true, y_pred):\n",
    "\ty_pred_masked = tf.math.multiply(y_pred, y_land_mask)\n",
    "\ty_true_masked = tf.math.multiply(y_true, y_land_mask)\n",
    "\tsquared_resids = tf.square(y_true_masked - y_pred_masked)\n",
    "\tloss = tf.reduce_mean(squared_resids)\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb829aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, Conv2D, ConvLSTM2D, BatchNormalization, UpSampling2D,MaxPooling2D, concatenate, Flatten, Reshape\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.models import load_model, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f602c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1_shape = (12, 448, 304, 5)\n",
    "input2_shape = (24, 448, 304, 5)\n",
    "\n",
    "learning_rate=1e-4\n",
    "filter_size=3\n",
    "use_temp_scaling=False\n",
    "n_output_classes=1\n",
    "loss = \"mse\"\n",
    "metrics = RootMeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be40fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    input1 = Input(shape=input1_shape)\n",
    "    input2 = Input(shape=input2_shape)\n",
    "\n",
    "    convlstm1 = ConvLSTM2D(8, (5,5), padding=\"same\", return_sequences=False, data_format=\"channels_last\")(input1)\n",
    "    \n",
    "    conv1 = Conv2D(np.int(16), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(convlstm1)\n",
    "    conv1 = Conv2D(np.int(16), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    bn1 = BatchNormalization(axis=-1)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(bn1)\n",
    "\n",
    "    convlstm2 = ConvLSTM2D(8, (5,5), padding=\"same\", return_sequences=False, data_format=\"channels_last\")(input2)\n",
    "    conv2 = Conv2D(np.int(32), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(convlstm2)\n",
    "    conv2 = Conv2D(np.int(32), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    bn2 = BatchNormalization(axis=-1)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(bn2)\n",
    "\n",
    "    conv3 = Conv2D(np.int(64), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(np.int(64), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    bn3 = BatchNormalization(axis=-1)(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(bn3)\n",
    "\n",
    "    up8 = Conv2D(np.int(32), 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2,2), interpolation='nearest')(pool3))\n",
    "    merge8 = concatenate([bn3,up8], axis=3)\n",
    "    conv8 = Conv2D(np.int(32), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "    conv8 = Conv2D(np.int(32), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "    bn8 = BatchNormalization(axis=-1)(conv8)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(bn8)\n",
    "\n",
    "    up9 = Conv2D(np.int(16), 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2,2), interpolation='nearest')(bn8))\n",
    "    merge9 = concatenate([conv1,up9], axis=3)\n",
    "    conv9 = Conv2D(np.int(16), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "    conv9 = Conv2D(np.int(16), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Conv2D(np.int(16), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "    output = Conv2D(n_output_classes, 1, activation='linear')(conv9)\n",
    "        \n",
    "    model = Model(inputs = [input1, input2], outputs = output)\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss=loss, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc24ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcb024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define early stopping callback\n",
    "early_stopping = EarlyStopping(patience=100, restore_best_weights=True)\n",
    "\n",
    "# fit model\n",
    "history = model.fit(x=[x_train, x_train2], y=y_train,epochs=50,batch_size=2,validation_split=.2,verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6630cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([x_test,x_test2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb084e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert scaling for forecasted values \n",
    "inv_y_pred = scaler_l.inverse_transform(y_pred.reshape(-1,1))\n",
    "\n",
    "# invert scaling for actual values\n",
    "inv_y_test = scaler_l.inverse_transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d852fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_y_pred = inv_y_pred.reshape(len(y_pred),448,304)\n",
    "print(inv_y_pred.shape)\n",
    "inv_y_test = inv_y_test.reshape(len(y_test),448,304)\n",
    "print(inv_y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape y_land_mask\n",
    "land_mask = np.load(\"data/y_land_mask_actual.npy\",allow_pickle=True)\n",
    "y_land_mask = land_mask.reshape(1, 448, 304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d2c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_masked = np.multiply(inv_y_pred, y_land_mask)\n",
    "y_true_masked = np.multiply(inv_y_test, y_land_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31265ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "rmse = sqrt(mean_squared_error(inv_y_test.flatten(), inv_y_pred.flatten()))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "r_sq = r2_score(inv_y_test.flatten(), inv_y_pred.flatten())\n",
    "print('Test R_Square: %.3f' % r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe44419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Post-Process RMSE\n",
    "post_y = np.clip(y_pred_masked, a_min = 0, a_max = 100)\n",
    "rmse1 = sqrt(mean_squared_error(y_true_masked.flatten(), post_y.flatten()))\n",
    "print('Post-Processed RMSE: %.3f' % rmse1)\n",
    "\n",
    "r_sq = r2_score(y_true_masked.flatten(), post_y.flatten())\n",
    "print('Post-Processed R_Square: %.3f' % r_sq)\n",
    "\n",
    "mae = mean_absolute_error(y_true_masked.flatten(), post_y.flatten())\n",
    "print('Post-Processed MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/pred_ice_mt_icnet_lag6.npy\", \"wb\") as f:\n",
    "  np.save(f, post_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c7d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "fig, ax = pyplot.subplots(figsize=(8,6))\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='valid')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda SIC",
   "language": "python",
   "name": "anaconda-sea-ice-research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
